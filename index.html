<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image">
  <meta name="keywords" content="ControlTac, tactile, generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image</title>

  <!-- Preconnect for better performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <style>
    :root {
      --primary-color: #6366f1;
      --accent-color: #8b5cf6;
      --secondary-color: #06b6d4;
      --light-purple: #f3f4f6;
      --dark-purple: #4338ca;
      --gradient-start: #667eea;
      --gradient-end: #764ba2;
      --text-primary: #1f2937;
      --text-secondary: #6b7280;
      --border-color: #e5e7eb;
      --shadow-soft: 0 10px 25px rgba(99, 102, 241, 0.1);
      --shadow-medium: 0 20px 40px rgba(99, 102, 241, 0.15);
      --bg-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      --card-bg: rgba(255, 255, 255, 0.95);
    }

    * {
      scroll-behavior: smooth;
    }

    body {
      font-family: 'Inter', sans-serif;
      line-height: 1.6;
      color: var(--text-primary);
      background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
      min-height: 100vh;
    }

    /* Enhanced navbar with glassmorphism */
    .navbar {
      background: rgba(255, 255, 255, 0.9);
      backdrop-filter: blur(20px);
      border-bottom: 1px solid rgba(99, 102, 241, 0.1);
      position: sticky;
      top: 0;
      z-index: 1000;
      transition: all 0.3s ease;
      box-shadow: 0 4px 20px rgba(99, 102, 241, 0.1);
    }

    .navbar-brand {
      padding: 0 1rem;
    }

    .navbar-item {
      font-weight: 500;
      transition: all 0.3s ease;
      color: var(--text-primary);
    }

    .navbar-item:hover {
      background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
      color: white;
      border-radius: 8px;
      transform: translateY(-1px);
    }

    /* Hero section with modern gradient */
    .hero-section {
      position: relative;
      color: white;
      padding: 3rem 0 2rem;
      overflow: hidden;
      min-height: 70vh;
      display: flex;
      align-items: center;
      background-image: url('./static/images/front.gif');
      background-size: cover;
      background-position: center;
      background-repeat: no-repeat;
    }

    .hero-section::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: linear-gradient(135deg, rgba(0, 0, 0, 0.4) 0%, rgba(0, 0, 0, 0.6) 100%);
      z-index: 1;
    }

    .hero-section::after {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><defs><pattern id="grain" width="100" height="100" patternUnits="userSpaceOnUse"><circle cx="25" cy="25" r="1" fill="white" opacity="0.1"/><circle cx="75" cy="75" r="1" fill="white" opacity="0.1"/><circle cx="50" cy="10" r="0.5" fill="white" opacity="0.15"/><circle cx="10" cy="60" r="0.5" fill="white" opacity="0.15"/><circle cx="90" cy="40" r="0.5" fill="white" opacity="0.15"/></pattern></defs><rect width="100" height="100" fill="url(%23grain)"/></svg>');
      z-index: 2;
      pointer-events: none;
    }

    .hero-content {
      position: relative;
      z-index: 3;
      max-width: 800px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    .publication-title {
      font-weight: 700;
      font-size: 3rem;
      line-height: 1.2;
      margin-bottom: 2rem;
      text-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      background: linear-gradient(45deg, #ffffff, #e0e7ff);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .name {
      background: linear-gradient(45deg, #fbbf24, #f59e0b);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      font-weight: 800;
      text-shadow: none;
      font-variant: small-caps;
      text-shadow: 0 0 20px rgba(251, 191, 36, 0.3);
    }

    .publication-authors {
      text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
      font-size: 1.2rem;
      margin-bottom: 1rem;
    }

    /* Video section with glassmorphism */
    .video-hero {
      background: var(--card-bg);
      backdrop-filter: blur(20px);
      padding: 4rem 0;
      position: relative;
      border-top: 1px solid rgba(99, 102, 241, 0.1);
      border-bottom: 1px solid rgba(99, 102, 241, 0.1);
    }

    .video-container {
      position: relative;
      width: 100%;
      max-width: 700px;
      margin: 0 auto;
      background: #000;
      border-radius: 20px;
      overflow: hidden;
      box-shadow: var(--shadow-medium);
      aspect-ratio: 16 / 9;
      transition: all 0.3s ease;
      border: 2px solid transparent;
      background-clip: padding-box;
    }

    .video-container::before {
      content: '';
      position: absolute;
      top: -2px;
      left: -2px;
      right: -2px;
      bottom: -2px;
      background: linear-gradient(45deg, var(--primary-color), var(--accent-color), var(--secondary-color));
      border-radius: 22px;
      z-index: -1;
      opacity: 0;
      transition: opacity 0.3s ease;
    }

    .video-container:hover::before {
      opacity: 1;
    }

    .video-container:hover {
      transform: translateY(-8px) scale(1.02);
      box-shadow: 0 30px 60px rgba(99, 102, 241, 0.25);
    }

    .video-container iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border: none;
      border-radius: 18px;
    }

    .video-description {
      max-width: 700px; 
      margin: 3rem auto 0;
      padding: 2rem;
      text-align: center;
      background: var(--card-bg);
      backdrop-filter: blur(20px);
      border-radius: 16px;
      box-shadow: var(--shadow-soft);
      border: 1px solid rgba(99, 102, 241, 0.1);
      color: var(--text-secondary);
      font-size: 1.1rem;
      line-height: 1.8;
    }

    /* Teaser section with enhanced cards */
    .teaser-section {
      background: linear-gradient(135deg, #fafafa 0%, #ffffff 100%);
      padding: 5rem 0;
      position: relative;
    }

    .teaser-content {
      display: flex;
      flex-direction: column;
      gap: 3rem;
      align-items: center;
      max-width: 800px; 
      margin: 0 auto;
      padding: 0 2rem;
    }

    .teaser-image {
      width: 100%;
      max-width: 800px; 
      border-radius: 24px;
      box-shadow: var(--shadow-medium);
      transition: all 0.3s ease;
      display: block;
      margin: 0 auto;
      border: 3px solid transparent;
      background-clip: padding-box;
    }

    .teaser-image:hover {
      transform: scale(1.03) rotate(1deg);
      box-shadow: 0 30px 60px rgba(99, 102, 241, 0.2);
    }

    .teaser-description {
      background: var(--card-bg);
      backdrop-filter: blur(20px);
      padding: 2.5rem;
      border-radius: 20px;
      box-shadow: var(--shadow-soft);
      border: 1px solid rgba(99, 102, 241, 0.1);
      max-width: 750px; 
      text-align: left;
      position: relative;
      overflow: hidden;
    }

    .teaser-description::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 4px;
      background: linear-gradient(90deg, var(--primary-color), var(--accent-color), var(--secondary-color));
      border-radius: 20px 20px 0 0;
    }

    /* Enhanced buttons */
    .publication-links {
      display: flex;
      gap: 1.5rem;
      justify-content: center;
      flex-wrap: wrap;
      margin-top: 2.5rem;
    }

    .button.is-rounded {
      background: rgba(255, 255, 255, 0.15);
      border: 2px solid rgba(255, 255, 255, 0.2);
      color: white;
      font-weight: 600;
      transition: all 0.3s ease;
      backdrop-filter: blur(15px);
      text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
      padding: 0.75rem 1.5rem;
      font-size: 1rem;
      position: relative;
      overflow: hidden;
    }

    .button.is-rounded::before {
      content: '';
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.1), transparent);
      transition: left 0.5s ease;
    }

    .button.is-rounded:hover::before {
      left: 100%;
    }

    .button.is-rounded:hover {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.4);
      transform: translateY(-3px);
      box-shadow: 0 15px 30px rgba(255, 255, 255, 0.2);
    }

    .button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    /* Enhanced content sections */
    .content-section {
      background: var(--card-bg);
      backdrop-filter: blur(20px);
      margin: 3rem 0;
      border-radius: 24px;
      box-shadow: var(--shadow-soft);
      border: 1px solid rgba(99, 102, 241, 0.1);
      position: relative;
      overflow: hidden;
    }

    .content-section::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 4px;
      background: linear-gradient(90deg, var(--primary-color), var(--accent-color), var(--secondary-color));
      border-radius: 24px 24px 0 0;
    }

    .section-title {
      font-size: 2.5rem;
      font-weight: 700;
      color: var(--text-primary);
      margin-bottom: 2rem;
      position: relative;
      background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .section-title::after {
      content: '';
      position: absolute;
      bottom: -12px;
      left: 50%;
      transform: translateX(-50%);
      width: 80px;
      height: 4px;
      background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
      border-radius: 2px;
    }

    /* Content sections styling */
    .abstract-section, .method-section, .visualization-section, .downstream-section {
      background: var(--card-bg);
      backdrop-filter: blur(20px);
      padding: 4rem 0;
      position: relative;
      border-top: 1px solid rgba(99, 102, 241, 0.1);
      border-bottom: 1px solid rgba(99, 102, 241, 0.1);
    }

    .content-wrapper {
      max-width: 900px; 
      margin: 0 auto;
      padding: 0 2rem;
    }

    /* Typography improvements */
    .content p {
      font-size: 1.1rem;
      line-height: 1.9;
      color: var(--text-primary);
      margin-bottom: 2rem;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }

    .subtitle.is-5 {
      color: var(--primary-color);
      font-weight: 600;
      margin-top: 3rem;
      margin-bottom: 1rem;
      font-size: 1.4rem;
    }

    /* Enhanced image containers */
    .gif-container, .image-container {
      text-align: center;
      margin: 3rem auto;
      display: flex;
      justify-content: center;
      align-items: center;
      max-width: 700px; 
    }

    .gif-container img, .image-container img {
      border-radius: 16px;
      box-shadow: var(--shadow-soft);
      transition: all 0.3s ease;
      max-width: 100%;
      height: auto;
      display: block;
      border: 2px solid transparent;
      background-clip: padding-box;
    }

    .gif-container img:hover, .image-container img:hover {
      transform: scale(1.05);
      box-shadow: var(--shadow-medium);
    }

    /* BibTeX section */
    #BibTeX {
      background: var(--card-bg);
      backdrop-filter: blur(20px);
      border-radius: 20px;
      margin: 3rem auto;
      padding: 2.5rem;
      max-width: 800px;
      text-align: left;
      box-shadow: var(--shadow-soft);
      border: 1px solid rgba(99, 102, 241, 0.1);
      position: relative;
      overflow: hidden;
    }
    
    #BibTeX::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 4px;
      background: linear-gradient(90deg, var(--primary-color), var(--accent-color), var(--secondary-color));
      border-radius: 20px 20px 0 0;
    }
    
    #BibTeX h2 {
      text-align: center;
      margin-bottom: 2rem;
    }
    
    #BibTeX pre {
      background: linear-gradient(135deg, #f8fafc, #f1f5f9);
      border: 1px solid rgba(99, 102, 241, 0.2);
      border-radius: 12px;
      padding: 1.5rem;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.85rem;
      overflow-x: auto;
      white-space: pre; 
      line-height: 1.6;
      color: #1e293b;
      box-shadow: inset 0 2px 4px rgba(99, 102, 241, 0.1);
    }

    /* Footer styling */
    .footer {
      background: linear-gradient(135deg, var(--text-primary), #374151);
      color: white;
      padding: 4rem 0 3rem;
      margin-top: 5rem;
      position: relative;
    }

    .footer::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 4px;
      background: linear-gradient(90deg, var(--primary-color), var(--accent-color), var(--secondary-color));
    }

    /* Center all images by default with size constraints */
    img {
      display: block;
      margin: 0 auto;
      max-width: 700px; 
      height: auto;
    }

    .narrow-content {
      max-width: 700px;
      margin: 0 auto;
      padding: 0 1rem;
    }

    .extra-narrow-content {
      max-width: 600px;
      margin: 0 auto;
      padding: 0 1rem;
    }

    /* Responsive design improvements */
    @media screen and (max-width: 768px) {
      .hero-section {
        padding: 3rem 0 2rem;
        min-height: 70vh;
      }

      .hero-content {
        padding: 0 1.5rem;
      }

      .publication-title {
        font-size: 2rem;
        margin-bottom: 1.5rem;
      }

      .video-hero {
        padding: 3rem 0;
      }

      .video-container {
        margin: 0 1rem;
        border-radius: 16px;
        max-width: calc(100% - 2rem);
      }

      .video-description {
        font-size: 1rem;
        margin-top: 2rem;
        max-width: calc(100% - 2rem);
        padding: 1.5rem;
      }

      .teaser-section {
        padding: 3rem 0;
      }

      .teaser-content {
        padding: 0 1.5rem;
        max-width: calc(100% - 3rem);
      }

      .teaser-description {
        margin: 0 0;
        padding: 2rem;
        max-width: 100%;
      }

      .publication-links {
        flex-direction: column;
        align-items: center;
        gap: 1rem;
      }

      .button.is-rounded {
        width: 250px;
      }

      .section-title {
        font-size: 2rem;
      }

      .content-wrapper {
        padding: 0 1.5rem;
      }

      img {
        max-width: 100%;
      }
    }

    /* Smooth animations */
    .fade-in {
      animation: fadeIn 1s ease-out;
    }

    @keyframes fadeIn {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    /* Loading animation for placeholder elements */
    .loading-placeholder {
      background: linear-gradient(90deg, #f0f0f0 25%, #e0e0e0 50%, #f0f0f0 75%);
      background-size: 200% 100%;
      animation: loading 2s infinite;
    }

    @keyframes loading {
      0% {
        background-position: 200% 0;
      }
      100% {
        background-position: -200% 0;
      }
    }
  </style>
</head>
<body>



<!-- Hero Section -->
<section class="hero-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered hero-content">
        <h1 class="publication-title fade-in">
          <span class="name">ControlTac</span>: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image
        </h1>

        <div class="is-size-5 publication-authors fade-in">
          <span class="author-block">Submission Number: 13062</span>
        </div>

        

        
          <span class="link-block">
            <div class="button is-normal is-rounded" style="cursor: default;">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code (Available Upon Acceptance)</span>
            </div>
          </span>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Video Section -->
<section class="video-hero fade-in">
  <div class="container is-max-desktop">
    <div class="has-text-centered">
      <div class="video-container">
        <video 
          src="./static/images/video.mp4" 
          controls 
          preload="metadata"
          class="video-player"
        >
          Your browser does not support HTML5 video playback.
        </video>
      </div>
      <div class="video-description">
        <strong>Introduction Video:</strong> This video provides an overview of our ControlTac method, demonstrating its capabilities in controllable tactile data generation for robotic applications.
      </div>
    </div>
  </div>
</section>



<!-- Teaser Section -->
<section class="teaser-section fade-in">
  <div class="container is-max-desktop">
    <div class="teaser-content">
      <img src="./static/images/teaser.png" alt="ControlTac Teaser" class="teaser-image responsive-image" />
      <div class="teaser-description">
        <p>
          <strong>At a Glance:</strong> Starting from a single reference image, <span class="name" style="color: var(--primary-color);">ControlTac</span> generates thousands of 
          augmented tactile images with controllable contact forces and positions <em>(Left)</em>. 
          These generated images prove highly effective for downstream tasks <em>(Middle)</em> and demonstrate practical utility 
          in real-world robotic experiments <em>(Right)</em>.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Abstract Section -->
<section class="abstract-section">
  <div class="content-wrapper">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="section-title">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-based tactile sensing is widely used in perception, reconstruction, and robotic manipulation, 
            yet collecting large-scale tactile data remains costly due to diverse sensor-object interactions and 
            inconsistencies across sensor instances. Existing approaches to scaling tactile data—simulation and 
            free-form tactile generation—often yield unrealistically rendered signals with poor transfer to highly 
            dynamic real-world tasks. We propose <span class="name" style="color: var(--primary-color);">ControlTac</span>, a two-stage controllable framework that generates 
            realistic tactile images conditioned on a single reference tactile image, contact force, and contact position. 
            By grounding generation in these important physical priors, <span class="name" style="color: var(--primary-color);">ControlTac</span> produces realistic samples that 
            effectively capture task-relevant variations. Across three downstream tasks and three real-world experiments, 
            the augmented datasets using our approach consistently improve performance and demonstrate practical utility 
            in dynamic real-world settings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method Section -->
<section class="method-section">
  <div class="content-wrapper">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="section-title">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            <span class="name" style="color: var(--primary-color);">ControlTac</span> consists of two key components:
            <strong>a. Force-Control:</strong> We input the background-removed tactile image <em>x</em> into the DiT model, conditioned on the 3D contact force <em>ΔF</em>, to generate force-specific tactile variations.
            <strong>b. Position-Control:</strong> We transfer the pretrained DiT from stage one and fine-tune it using ControlNet, conditioned on a contact mask <em>c</em>, to synthesize realistic tactile images <em>y<sub>B</sub></em> under different contact positions and forces.
          </p>
        </div>

        <div class="image-container">
          <img src="./static/images/framework.png" alt="ControlTac Framework" class="responsive-image">
        </div>
        
        <div class="content has-text-left" style="margin-top: 3rem;">
          <p>Here, we demonstrate how to annotate the contact mask to represent the contact position.</p>
          <div class="gif-container">
            <img src="./static/images/mask.gif" alt="Contact Mask Annotation" class="responsive-image" style="max-width: 80%;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Visualization Section -->
<section class="visualization-section">
  <div class="content-wrapper">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="section-title">Visualization</h2>

        <h3 class="subtitle is-5">Qualitative Comparison</h3>
        <div class="content has-text-justified">
          <p>
            We conduct a qualitative comparison between <span class="name" style="color: var(--primary-color);">ControlTac</span> and other generators and simulators. <span class="name" style="color: var(--primary-color);">ControlTac</span> exhibits superior realism, variation, and controllability in the generated tactile images.
          </p>
          <div class="image-container">
            <img src="./static/images/vis1.png" alt="Comparison with other methods" class="responsive-image">
          </div>
        </div>

        <h3 class="subtitle is-5">Comparison with Baseline Models</h3>
        <div class="content has-text-justified">
          <<p>
            The first column shows 3D previews of six objects, followed by the input tactile image (Ref. Image)
            in the second column and the Contact Mask in the third column. The fourth column displays the initial
            force (top) and target force (bottom). Subsequent columns present the Ground Truth (G.T.) and results
            from <span class="name" style="color: var(--primary-color);">ControlTac</span>, the hybrid force-position conditional diffusion model (Hybrid), the separate-control
            pipeline (Separate), and simulation results from
            <a href="https://arxiv.org/pdf/2109.04027.pdf" target="_blank" rel="noopener noreferrer">
              Taxim (Si &amp; Yuan, 2022)
            </a>.
            In the upper part, we visualize the generated images for comparison; in the lower part, we show the
            error maps highlighting differences from the ground-truth tactile image.
          </p>
          <div class="image-container">
            <img src="./static/images/vis2.png" alt="Baseline comparison" class="responsive-image">
          </div>
        </div>

        <h3 class="subtitle is-5">Force-Controlled and Position-Controlled Generation</h3>
        <div class="content has-text-justified">
          <p>
            The figure below showcases the generation results of force-controlled and position-controlled components in <span class="name" style="color: var(--primary-color);">ControlTac</span>.
          </p>
          <div class="gif-container">
            <img src="./static/images/vis3.gif" alt="Force and position control demonstration" class="responsive-image" style="max-width: 80%;">
          </div>
        </div>

        <h3 class="subtitle is-5">Diversity of Generated Tactile Images</h3>
        <div class="content has-text-justified">
          <p>
            The figure below clearly demonstrates that <span class="name" style="color: var(--primary-color);">ControlTac</span> can generate a diverse range of tactile images from a single reference tactile image.
          </p>
          <div class="gif-container">
            <img src="./static/images/vis4_new.gif" alt="Diversity demonstration" class="responsive-image">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Downstream Tasks Section -->
<section class="downstream-section">
  <div class="content-wrapper">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="section-title">Downstream Tasks</h2>

        <h3 class="subtitle is-5">Force Estimation</h3>
        <div class="content has-text-justified">
          <p>
            The figure below demonstrates that <span class="name" style="color: var(--primary-color);">ControlTac</span> can cover the variation of positions and force, and remarkably improves MAE even 
            with small real subsets. With only a third of the real data, the performance can reach a competitive performance 
            to the full dataset, where the performance with only real data is much worse since it cannot cover all the variations 
            of forces and positions. It is worthy to note that combining all real + generated data performs slightly worse than 
            using only real data, and this is because FeelAnyForce already achieves near-oracle performance with full forces and 
            positions coverage, although it's challenging to collect them in the real world. 
          </p>
          <div class="image-container">
            <img src="./static/images/force1.png" alt="Force estimation results" class="responsive-image">
          </div>
          <p>
            We further validate the effectiveness of <span class="name" style="color: var(--primary-color);">ControlTac</span> in real-world pushing 
            experiments. The force estimator trained only with generated tactile data achieves
            comparable performance to the one trained on real tactile data, demonstrating that the generated data is 
            realistic and reliable enough to be used directly for training in practical scenarios.
          </p>
          <div class="gif-container">
            <img src="./static/images/force2.gif" alt="Real-world pushing experiments" class="responsive-image" style="max-width: 80%;">
          </div>
        </div>

        <h3 class="subtitle is-5">Position Estimation</h3>
        <div class="content has-text-justified">
          <p>
            As shown in the table below, pose estimators trained solely on tactile images generated by <span class="name" style="color: var(--primary-color);">ControlTac</span> 
            achieve strong performance across all objects, including the unseen T Shape and USB with the new sensor 
            sample. Remarkably, using the same amount of generated data outperforms training on real data alone, even 
            when the real dataset is relatively large, as capturing tactile data that fully covers all contact variations 
            in the dynamic real world is extremely challenging.  In such case, generated  data proves particularly valuable 
            since all the covered positions can be generated.
          </p>
          <p>
            Furthermore, <span class="name" style="color: var(--primary-color);">ControlTac</span> not only outperforms simulation-based data from 
            <a href="https://arxiv.org/pdf/2109.04027.pdf" target="_blank" rel="noopener noreferrer">
              Taxim (Si &amp; Yuan, 2022)
            </a>, 
            where simulated images are not realistic, but also surpasses traditional PCA-based
            <a href="https://journals.sagepub.com/doi/full/10.1177/02783649211027233" target="_blank" rel="noopener noreferrer">
               (She et al., 2021)
            </a>
            pose estimation methods. We also evaluate the pose estimator 
            under varying versus fixed forces (denoted as “fixed” in Table set to the median value of 6.5 N). 
            Results show that unfixed force improves performance since it covers the force variations in the real-world scenarios.
          </p>
          <div class="image-container">
            <img src="./static/images/pos1.png" alt="Pose estimation comparison" class="responsive-image" style="max-width: 50%;">
          </div>
          <p>
            To further evaluate the performance of the pose estimator trained with <span class="name" style="color: var(--primary-color);">ControlTac</span>-generated data, 
            we conducted a real-time pose tracking experiment. Our model successfully tracked poses at a 
            frequency of 10 Hz, highlighting its practicality in dynamic real-world scenarios.
          </p>
          <div class="gif-container">
            <img src="./static/images/tracking.gif" alt="Real-time pose tracking" class="responsive-image">
          </div>
          <p>
            In the Precise Insertion task, the pose estimator trained with <span class="name" style="color: var(--primary-color);">ControlTac</span>-generated 
            data achieved success rates of 90% on the cylinder and 85% on the cross. Notably, it achieved success rates of 85% on the unseen T-shape and 75% on the Type-C connector.
          </p>
          <div class="gif-container">
            <img src="./static/images/insertion.gif" alt="Precise insertion task results" class="responsive-image" style="max-width: 50%;">
          </div>
        </div>

        <h3 class="subtitle is-5">Object Classification</h3>
        <div class="content has-text-justified">
          <p>
            In the object classification task, we found that compared to traditional augmentation methods, using <span class="name" style="color: var(--primary-color);">ControlTac</span> for data augmentation yields significantly better performance—whether with a simple CNN classifier, a ViT trained from scratch, or a ViT pretrained on ImageNet.
          </p>
          <p style="font-style: italic;">
            Note: G = geometric augmentation; C = color augmentation; Gen = our <span class="name" style="color: var(--primary-color);">ControlTac</span>-based augmentation method.
          </p>
          <div class="image-container">
            <img src="./static/images/classification.png" alt="Object classification results" class="responsive-image">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<script>
  // Navbar burger functionality for mobile
  document.addEventListener('DOMContentLoaded', () => {
    // Get all "navbar-burger" elements
    const navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

    // Add a click event on each of them
    navbarBurgers.forEach(el => {
      el.addEventListener('click', () => {
        // Get the target from the "data-target" attribute
        const target = el.dataset.target;
        const targetElement = document.getElementById(target);

        // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
        el.classList.toggle('is-active');
        targetElement.classList.toggle('is-active');
      });
    });

    // Add smooth scrolling and fade-in animations
    const observerOptions = {
      threshold: 0.1,
      rootMargin: '0px 0px -50px 0px'
    };

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('fade-in');
        }
      });
    }, observerOptions);

    // Observe all sections
    document.querySelectorAll('section').forEach(section => {
      observer.observe(section);
    });

    // Add parallax effect to hero section
    window.addEventListener('scroll', () => {
      const scrolled = window.pageYOffset;
      const heroSection = document.querySelector('.hero-section');
      if (heroSection) {
        heroSection.style.transform = `translateY(${scrolled * 0.5}px)`;
      }
    });
  });

  // Add loading animation for images
  document.addEventListener('DOMContentLoaded', () => {
    const images = document.querySelectorAll('img');
    images.forEach(img => {
      img.addEventListener('load', () => {
        img.style.opacity = '1';
        img.style.transform = 'scale(1)';
      });
      
      // Set initial styles
      img.style.opacity = '0';
      img.style.transform = 'scale(0.95)';
      img.style.transition = 'opacity 0.5s ease, transform 0.5s ease';
    });
  });
</script>

</body>
</html>
